#  Text-to-Image Generation using CLIP & Latent Diffusion (LDM)

This project is a Generative AI application that combines **CLIP (Contrastive Language‚ÄìImage Pretraining)** with a **Latent Diffusion Model (LDM)** to generate high-quality images from textual descriptions. It leverages the **MS-COCO dataset** for training and evaluation, and introduces a relevance scoring mechanism using CLIP to rank and validate the generated outputs.

---

## üìù Problem Statement

Despite recent advances in generative models, generating **visually coherent and semantically relevant** images from textual prompts remains a challenge, especially when constrained by limited compute resources and noisy supervision. Traditional GANs and earlier diffusion models lack fine-grained text-image alignment.

---

##  Project Goal

To build a GenAI project that:
- Generates images from natural language prompts.
- Uses a **CLIP-based scoring system** to ensure semantic alignment between the prompt and the image.
- Employs **Latent Diffusion Models** to reduce computational overhead.
- Trains and evaluates on the **COCO 2017 dataset** for real-world caption-image mapping.

---

##  My Role

This Project developed as part of a Generative AI . I was responsible for:
- Designing the pipeline architecture.
- Integrating CLIP and LDM models.
- Preparing and preprocessing the COCO dataset.
- Implementing prompt-image relevance scoring.
- Building a minimal UI to take text input and visualize outputs.

---

## üí° What Makes This Project Unique?

‚úÖ Uses **CLIP not just for inference**, but as an **evaluation and scoring mechanism** to improve output relevance.

‚úÖ Introduces a **relevance score** 

‚úÖ Uses **Latent Diffusion Model (LDM)** 
